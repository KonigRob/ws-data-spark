{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b626d968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22025"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, avg, stddev, max, count, min\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "df = spark.read.options(\n",
    "    header='True',\n",
    "    inferSchema='True',\n",
    "    delimiter=',',\n",
    ").csv(os.path.expanduser('~/data/DataSample.csv'))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1646131c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19999"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "df1 = df.dropDuplicates([' TimeSt','Latitude','Longitude'])\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f84ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPoi = spark.read.options(\n",
    "        header='True',\n",
    "        inferSchema='True',\n",
    "        delimiter=',',\n",
    "    ).csv(os.path.expanduser('~/data/POIList.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf9934a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently one of the points of interest is a duplicate, and the Lat has a whitespace infront of it.  \n",
    "# Also renaming the column names as they're too similar to the datasheets names.\n",
    "dfPoi = dfPoi.dropDuplicates([' Latitude','Longitude'])\n",
    "dfPoi = dfPoi.withColumnRenamed(' Latitude','poiLat').withColumnRenamed('Longitude', 'poiLong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19092e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula stolen from \n",
    "# https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "def calcDistance(rLat, rLong, tLat, tLong):\n",
    "    print(rLat)\n",
    "    lon1 = math.radians(rLat)\n",
    "    lat1 = math.radians(rLong)\n",
    "    lon2 = math.radians(tLat)\n",
    "    lat2 = math.radians(tLong)\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371 #Radius of earth in kilometers. Not going to use 3956 for miles since Canada. :>\n",
    "    b = c * r\n",
    "    return b\n",
    "\n",
    "cCalcDistance = udf(lambda a,b,c,d:calcDistance(a,b,c,d))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b84577a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+----+--------+---------+-----+------+-------+---+-------------+\n",
      "| TimeSt|Country|Province|City|Latitude|Longitude|POIID|poiLat|poiLong|_ID|min(distance)|\n",
      "+-------+-------+--------+----+--------+---------+-----+------+-------+---+-------------+\n",
      "+-------+-------+--------+----+--------+---------+-----+------+-------+---+-------------+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So my first attempt at this one, was to take the data, use a withColumn function to pass it in, along with the\n",
    "# Points of Interest spots, and then compare each poi with the lat and long, and see which is the closest.\n",
    "# But I learned that you can't nest withColumn functions as it looks like they lock the thread to increase the\n",
    "# effeicency for applying the function to the data.  Which means I couldn't use that approach.\n",
    "\n",
    "# So I thought about it, and while this isn't as scalable, it'll do: to apply the lat and long of each request to all the\n",
    "# poi's, then by grouping by their original ID, you can find out which had the lowest distance, and then just grab the\n",
    "# poi number itself.  And then delete the others.\n",
    "\n",
    "# One issue I had was that the udf returns a string for some reason?  Was able to bypass that by using withColumn to apply \n",
    "# all the data to be casted as a DoubleType() and delete the previous column all in one go. (not technical terms, but will do)\n",
    "\n",
    "# Question 2\n",
    "dfPoi = dfPoi.select('POIID',col('poiLat'), col('poiLong'))\n",
    "df1=df1.crossJoin(dfPoi)\n",
    "df1 = df1.withColumn('distance', cCalcDistance(df1['Latitude'],df1['Longitude'], df1['poiLat'], df1['poiLong']))\n",
    "df1 = df1.withColumn('distance', df1['distance'].cast(DoubleType()))\n",
    "\n",
    "tmpDf1 = df1.groupBy('_ID').min('distance')\n",
    "df1=df1.join(tmpDf1,(df1['_ID'] == tmpDf1['_ID']) & (df1['distance'] == tmpDf1['min(distance)'])).drop(tmpDf1._ID)\n",
    "df1=df1.drop('distance')\n",
    "df1.show(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4f55639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df6 = df1.groupBy('POIID')\n",
    "# alt.Chart(df6).mark_circle(\n",
    "#     color='blue',\n",
    "#     opacity=0.3\n",
    "# ).encode(\n",
    "# x='Latitude',\n",
    "# y='Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4603d375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----+--------------------+\n",
      "|POIID|            Radius|Count|             Density|\n",
      "+-----+------------------+-----+--------------------+\n",
      "| POI4|12784.457896075026|  477|9.289746689640268E-7|\n",
      "| POI1|14376.059009320134| 9704|1.494587073110212...|\n",
      "| POI3| 2210.180854641291| 9818|6.397606583284486E-4|\n",
      "+-----+------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 3 a\n",
    "df3 = df1.groupBy('POIID').agg(avg('min(distance)').alias('Average distance'), stddev('min(distance)').alias('Standard Deviation'))\n",
    "\n",
    "#3 b was not the best for the wording, and I spend many hours on attempting to draw a circle to visualize the data.  \n",
    "\n",
    "# Question 3 b\n",
    "df3b = df1.groupBy('POIID').agg(max('min(distance)').alias('Radius'), count('min(distance)').alias('Count'))\n",
    "df3b = df3b.withColumn('Density', df3b['Count']/(df3b['Radius']**2*math.pi))\n",
    "df3b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ba218a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010498152119975096\n",
      "+-----+------------------+-----+--------------------+-----------------+\n",
      "|POIID|            Radius|Count|             density|density(adjusted)|\n",
      "+-----+------------------+-----+--------------------+-----------------+\n",
      "| POI4|1183.5582054834815|  462|1.049815211997509...|            -10.0|\n",
      "| POI1|1401.4928282958247| 9386|0.001521069008350...|5.272995535688684|\n",
      "| POI3|1248.9641974323345| 9602|0.001959349230326935|9.999999999999996|\n",
      "+-----+------------------+-----+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So basically I need to get the IQR, then filter the data that falls inside that range\n",
    "# group that data by POIID, and create new columns for radius without the outliers, and see how many there are\n",
    "# Then find the density by deviding the count by the area of the circle\n",
    "\n",
    "# To get the data and rescale it to have the selected min and max ranges, I'll be using \n",
    "# newvalue= (b-a)*(x-minimum)/(maximum-minimum)+a\n",
    "# (taken after some googling from https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range and modifying)\n",
    "# \n",
    "\n",
    "# Question 4 a\n",
    "Q1 = df1.approxQuantile('min(distance)', [0.35], 0.05)\n",
    "Q3 = df1.approxQuantile('min(distance)', [0.65], 0.05)\n",
    "IQR = Q3[0] - Q1[0]\n",
    "lowerRange = Q1[0] - 1.5*IQR\n",
    "upperRange = Q3[0] + 1.5*IQR\n",
    "\n",
    "df4 = df1[(df1['min(distance)'] > lowerRange) & (df1['min(distance)'] < upperRange)].groupBy('POIID').agg(max('min(distance)').alias('Radius'), count('min(distance)').alias('Count'))\n",
    "df4 = df4.withColumn('density', df4['Count']/(df4['Radius']**2*math.pi))\n",
    "\n",
    "# now we use the model formula\n",
    "mini = df4.agg(min('density')).first()[0]\n",
    "maxi = df4.agg(max('density')).first()[0]\n",
    "a = -10\n",
    "b = 10\n",
    "print(mini)\n",
    "df4a = df4.withColumn('density(adjusted)', ((b - a) * (df4['density']-mini)/(maxi-mini)+a))\n",
    "df4a.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25c40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
